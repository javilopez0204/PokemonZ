import streamlit as st
import os
import time
from pypdf import PdfReader
from typing import List, Optional
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import PromptTemplate
from langchain_core.vectorstores import VectorStore

# ---------------------------------------------------------------------------
# Constantes
# ---------------------------------------------------------------------------
MODEL_NAME      = "gemini-2.5-flash-lite-preview-06-17"
EMBEDDING_MODEL = "all-MiniLM-L6-v2"
PAGE_ICON       = "âš¡"
PAGE_TITLE      = "PokÃ©dex Z â€“ Profesor Z (Gemini Edition)"
LOCAL_PDF_PATH  = "GuiaPokemonZ.pdf"

# Prompt principal: el LLM responde usando el contexto
QA_TEMPLATE = """Eres el "Profesor Z", la mÃ¡xima autoridad en PokÃ©mon Z.
Tienes acceso completo a la guÃ­a oficial del juego.

INSTRUCCIONES CRÃTICAS:
1. La respuesta SIEMPRE estÃ¡ en el "Contexto" de abajo. LÃ©elo entero con atenciÃ³n.
2. Busca keywords, sinÃ³nimos y variantes ortogrÃ¡ficas antes de rendirte.
3. Si encuentras la informaciÃ³n aunque sea parcial, dala.
4. Solo di "no encuentro esa informaciÃ³n" si el contexto estÃ¡ completamente vacÃ­o o irrelevante.
5. Responde siempre en **EspaÃ±ol**, con listas y negritas.

---
Contexto de la guÃ­a (lÃ©elo completo):
{context}

Historial:
{chat_history}

Pregunta: {question}

Respuesta del Profesor Z:"""

# Prompt de condensaciÃ³n: convierte preguntas de seguimiento en bÃºsquedas autÃ³nomas
# Usamos una versiÃ³n conservadora que preserva los tÃ©rminos originales
CONDENSE_TEMPLATE = """Dado el historial de conversaciÃ³n y la nueva pregunta del usuario,
reformula la pregunta para que sea autÃ³noma y mantenga TODOS los nombres propios,
tÃ©rminos de PokÃ©mon, objetos y lugares exactamente como aparecen.
Si la pregunta ya es autÃ³noma, devuÃ©lvela SIN cambios.

Historial:
{chat_history}

Pregunta original: {question}

Pregunta reformulada (conserva todos los tÃ©rminos clave):"""

# ---------------------------------------------------------------------------
# ConfiguraciÃ³n de pÃ¡gina
# ---------------------------------------------------------------------------
st.set_page_config(page_title=PAGE_TITLE, page_icon=PAGE_ICON, layout="wide")

# ---------------------------------------------------------------------------
# Backend â€“ procesamiento de PDF
# ---------------------------------------------------------------------------

def extract_text_from_pdf(path: str) -> str:
    try:
        reader = PdfReader(path)
        pages = []
        for i, page in enumerate(reader.pages):
            content = page.extract_text() or ""
            if content.strip():
                pages.append(f"[PÃ¡gina {i+1}]\n{content}")
        return "\n\n".join(pages)
    except Exception as exc:
        st.error(f"âŒ Error leyendo el PDF: {exc}")
        return ""


def split_text(text: str) -> List[str]:
    if not text.strip():
        return []
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,      # Chunks pequeÃ±os = recuperaciÃ³n mÃ¡s quirÃºrgica
        chunk_overlap=100,
        length_function=len,
        separators=["\n\n", "\n", ". ", " ", ""],
    )
    return splitter.split_text(text)


@st.cache_resource(show_spinner=False)
def load_embeddings() -> HuggingFaceEmbeddings:
    return HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)


@st.cache_resource(show_spinner="ğŸ“– El Profesor Z estÃ¡ memorizando la guÃ­a...")
def build_knowledge_base(file_path: str) -> Optional[tuple]:
    if not os.path.exists(file_path):
        return None
    text = extract_text_from_pdf(file_path)
    if not text:
        return None
    chunks = split_text(text)
    embeddings = load_embeddings()
    vectorstore = FAISS.from_texts(texts=chunks, embedding=embeddings)
    return vectorstore, chunks, text


def build_chain(vectorstore: VectorStore, chunks: List[str], api_key: str) -> ConversationalRetrievalChain:
    llm = ChatGoogleGenerativeAI(
        model=MODEL_NAME,
        google_api_key=api_key,
        temperature=0.0,   # MÃ¡xima fidelidad al contexto
        convert_system_message_to_human=True,
    )

    qa_prompt = PromptTemplate(
        input_variables=["context", "chat_history", "question"],
        template=QA_TEMPLATE,
    )

    condense_prompt = PromptTemplate(
        input_variables=["chat_history", "question"],
        template=CONDENSE_TEMPLATE,
    )

    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer",
    )

    # SemÃ¡ntico: alta k, sin MMR para no descartar chunks relevantes
    semantic_retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 25},
    )

    # BM25: captura coincidencias exactas de nombres propios y tÃ©rminos del juego
    bm25_retriever = BM25Retriever.from_texts(chunks)
    bm25_retriever.k = 25

    # HÃ­brido 50/50
    hybrid_retriever = EnsembleRetriever(
        retrievers=[semantic_retriever, bm25_retriever],
        weights=[0.5, 0.5],
    )

    return ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=hybrid_retriever,
        memory=memory,
        return_source_documents=True,
        combine_docs_chain_kwargs={"prompt": qa_prompt},
        condense_question_prompt=condense_prompt,   # ReformulaciÃ³n conservadora
        verbose=False,
    )


def direct_search(query: str, vectorstore: VectorStore, chunks: List[str], k: int = 10) -> List[str]:
    """BÃºsqueda directa sin LLM â€“ para diagnosticar quÃ© recupera el retriever."""
    sem_docs = vectorstore.similarity_search(query, k=k)
    sem_results = [d.page_content for d in sem_docs]

    bm25 = BM25Retriever.from_texts(chunks)
    bm25.k = k
    bm25_docs = bm25.get_relevant_documents(query)
    bm25_results = [d.page_content for d in bm25_docs]

    # UniÃ³n sin duplicados, preservando orden
    seen = set()
    combined = []
    for r in sem_results + bm25_results:
        if r not in seen:
            seen.add(r)
            combined.append(r)
    return combined


# ---------------------------------------------------------------------------
# Frontend â€“ estado de sesiÃ³n
# ---------------------------------------------------------------------------

def init_session():
    defaults = {
        "messages": [
            {
                "role": "assistant",
                "content": (
                    "Â¡Hola, entrenador! Soy el **Profesor Z** âš¡\n\n"
                    "He memorizado la guÃ­a completa de PokÃ©mon Z. "
                    "Â¡PregÃºntame lo que quieras!"
                ),
            }
        ],
        "conversation": None,
        "vectorstore": None,
        "chunks": [],
        "api_key": "",
        "total_queries": 0,
        "chunk_count": 0,
        "raw_text_len": 0,
        "debug_mode": False,
    }
    for key, val in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = val


# ---------------------------------------------------------------------------
# Frontend â€“ sidebar
# ---------------------------------------------------------------------------

def render_sidebar():
    with st.sidebar:
        st.image(
            "https://upload.wikimedia.org/wikipedia/commons/9/98/International_Pok%C3%A9mon_logo.svg",
            use_container_width=True,
        )
        st.header("âš™ï¸ ConfiguraciÃ³n")

        api_key = (
            st.secrets.get("GOOGLE_API_KEY", None)
            or os.environ.get("GOOGLE_API_KEY", None)
        )
        if api_key:
            st.session_state.api_key = api_key
            st.success("âœ… Google API Key configurada")
        else:
            typed_key = st.text_input(
                "Google API Key",
                type="password",
                placeholder="AIza...",
                help="ObtÃ©n tu clave en https://aistudio.google.com/",
            )
            if typed_key:
                st.session_state.api_key = typed_key
                st.success("âœ… Clave introducida")

        st.divider()

        # DiagnÃ³stico del PDF
        if os.path.exists(LOCAL_PDF_PATH):
            size_mb = os.path.getsize(LOCAL_PDF_PATH) / 1_048_576
            st.info(f"ğŸ“š **{LOCAL_PDF_PATH}** Â· {size_mb:.1f} MB")
        else:
            st.error(f"âŒ Falta el archivo `{LOCAL_PDF_PATH}`")

        if st.session_state.chunk_count > 0:
            st.success(f"ğŸ§© **{st.session_state.chunk_count}** fragmentos indexados")
            st.caption(f"ğŸ“ {st.session_state.raw_text_len:,} caracteres extraÃ­dos")
            if st.session_state.chunk_count < 50:
                st.warning("âš ï¸ Muy pocos fragmentos â€” posible PDF escaneado.")

        st.divider()

        # Modo debug
        st.session_state.debug_mode = st.toggle(
            "ğŸ” Modo Debug",
            value=st.session_state.debug_mode,
            help="Muestra los fragmentos exactos que el retriever enviÃ³ al modelo.",
        )

        # â”€â”€ Herramienta de bÃºsqueda directa â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Permite probar el retriever SIN el LLM para aislar el problema
        if st.session_state.vectorstore is not None:
            st.divider()
            st.markdown("#### ğŸ§ª BÃºsqueda directa en la guÃ­a")
            st.caption("Prueba el retriever sin el modelo. Si aparece la info aquÃ­ pero el chat no la usa, el problema es el prompt de condensaciÃ³n.")
            search_query = st.text_input("Buscar en la guÃ­a:", placeholder="Sylveon evoluciÃ³n")
            if search_query:
                results = direct_search(
                    search_query,
                    st.session_state.vectorstore,
                    st.session_state.chunks,
                    k=8,
                )
                if results:
                    for i, r in enumerate(results[:6], 1):
                        with st.expander(f"Resultado {i}"):
                            st.code(r, language=None)
                else:
                    st.warning("Sin resultados.")

        st.divider()

        col1, col2 = st.columns(2)
        col1.metric("ğŸ’¬ Mensajes", len(st.session_state.messages))
        col2.metric("ğŸ” Consultas", st.session_state.total_queries)

        if st.button("ğŸ—‘ï¸ Reiniciar conversaciÃ³n", use_container_width=True):
            st.session_state.messages = []
            st.session_state.total_queries = 0
            if st.session_state.conversation:
                st.session_state.conversation.memory.clear()
            st.rerun()

        st.divider()
        st.caption(
            f"Modelo: `{MODEL_NAME}`  \n"
            f"Embeddings: `{EMBEDDING_MODEL}`  \n"
            f"Retriever: HÃ­brido FAISS + BM25"
        )


# ---------------------------------------------------------------------------
# Frontend â€“ chat principal
# ---------------------------------------------------------------------------

def render_chat():
    st.title(f"{PAGE_ICON} Asistente PokÃ©mon Z")
    st.caption("Powered by Google Gemini Â· Retriever hÃ­brido semÃ¡ntico + BM25")

    # Inicializar RAG
    if st.session_state.api_key and st.session_state.conversation is None:
        if os.path.exists(LOCAL_PDF_PATH):
            with st.spinner("âš™ï¸ Inicializando sistema RAG..."):
                kb = build_knowledge_base(LOCAL_PDF_PATH)
                if kb:
                    vectorstore, chunks, raw_text = kb
                    st.session_state.vectorstore = vectorstore
                    st.session_state.chunks = chunks
                    st.session_state.chunk_count = len(chunks)
                    st.session_state.raw_text_len = len(raw_text)
                    st.session_state.conversation = build_chain(
                        vectorstore, chunks, st.session_state.api_key
                    )
                    st.rerun()
                else:
                    st.error("No se pudo procesar la guÃ­a.")
        else:
            st.warning("âš ï¸ El archivo `GuiaPokemonZ.pdf` no se encontrÃ³.")

    # Historial
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
            if st.session_state.debug_mode and msg.get("sources"):
                with st.expander("ğŸ” Fragmentos enviados al modelo"):
                    for i, src in enumerate(msg["sources"], 1):
                        st.markdown(f"**Fragmento {i}:**")
                        st.code(src, language=None)

    # Input
    if user_input := st.chat_input("Ej: Â¿CÃ³mo evoluciona Eevee a Sylveon?"):
        if not st.session_state.conversation:
            st.warning("â³ Introduce tu Google API Key y asegÃºrate de que el PDF estÃ¡ en su sitio.")
            return

        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        with st.chat_message("assistant"):
            placeholder = st.empty()
            placeholder.markdown("âš¡ _El Profesor Z estÃ¡ consultando la guÃ­aâ€¦_")
            t0 = time.perf_counter()
            try:
                result = st.session_state.conversation.invoke({"question": user_input})
                answer = result["answer"]
                sources = [doc.page_content for doc in result.get("source_documents", [])]
                elapsed = time.perf_counter() - t0

                placeholder.markdown(answer)
                st.caption(f"â±ï¸ {elapsed:.1f}s Â· {len(sources)} fragmentos consultados")

                if st.session_state.debug_mode and sources:
                    with st.expander("ğŸ” Fragmentos enviados al modelo"):
                        for i, src in enumerate(sources, 1):
                            st.markdown(f"**Fragmento {i}:**")
                            st.code(src, language=None)

                st.session_state.messages.append(
                    {"role": "assistant", "content": answer, "sources": sources}
                )
                st.session_state.total_queries += 1

            except Exception as exc:
                placeholder.error(f"âŒ Error: {exc}")


# ---------------------------------------------------------------------------
# Punto de entrada
# ---------------------------------------------------------------------------

def main():
    init_session()
    render_sidebar()
    render_chat()


if __name__ == "__main__":
    main()